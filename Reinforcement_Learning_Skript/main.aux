\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{ynt/global//global/global/global}
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{Contents\ttl@gmk {\@mkboth {\MakeUppercase    []{Contents}}{\MakeUppercase    []{Contents}}}}{3}{chapter*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Stochastic Bandits}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1}Two stage stochastic experiments}{5}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Introduction of stochastic Bandits}{6}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Algorithms: the exploration-exploitation trade-off}{14}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Basic committ-then-exploit algorithm}{14}{subsection.1.3.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Explore then Commit}}{14}{algocf.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Purely Greedy}{18}{subsection.1.3.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Purely greedy bandit algorithm}}{19}{algocf.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}$\epsilon $-greedy bandit algorithms}{19}{subsection.1.3.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces $\varepsilon $-greedy bandit algorithm}}{19}{algocf.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}UCB algorithm}{21}{subsection.1.3.4}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces UCB Algorithm }}{22}{algocf.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Boltzmann exploration}{26}{subsection.1.3.5}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {5}{\ignorespaces Simple Boltzmann exploration}}{27}{algocf.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Policy Gradient for Stochastic Bandits}{29}{subsection.1.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Markov Decision Model}{35}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1}Markov Decision Problem}{35}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Markov-Chains}{35}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Markov-Reward-Chains}{35}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Markov-Decision Process}{37}{subsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Stochastic Control theory}{45}{subsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Basic tabular Value Iteration}{57}{section.2.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {6}{\ignorespaces Basic Value Iteration}}{57}{algocf.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Basic Policy Iteration (Actor Critic) algorithm}{61}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Policy Evaluation}{61}{subsection.2.3.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {7}{\ignorespaces Iterative Policy Evaluation (Naive)}}{63}{algocf.7}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {8}{\ignorespaces Iterative Policy Evaluation (Totally Asynchronous Updates)}}{65}{algocf.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Policy Improvement}{65}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Policy Iteration algorithms (tabular actor critic)}{69}{subsection.2.3.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {9}{\ignorespaces Greedy Exact Policy Iteration (Actor-Critic)}}{69}{algocf.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Quick comparison of Value iteration and Policy Iteration}{70}{subsection.2.3.4}\protected@file@percent }
\newlabel{prop:mixing_comparison}{{3.4}{73}{}{tcb@cnt@mydefinition.13}{}}
\newlabel{prop:spectral_gap_policy_eval}{{3.4}{73}{}{tcb@cnt@mydefinition.14}{}}
\newlabel{prop:sequence_policy}{{3.4}{74}{}{tcb@cnt@mydefinition.15}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Simulation based dynamic Programming methods}{75}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1}A first example}{76}{section.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \relax }}{76}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: Map}{{3.1}{76}{\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Monte Carlo Policy evaluation and control}{76}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}First Visit Monte Carlo Policy Evaluation}{77}{subsection.3.2.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {10}{\ignorespaces First-visit Monte Carlo policy evaluation of $V^\pi $}}{78}{algocf.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Generalised policy iteration with first visit Monte Carlo estima- tion}{80}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Finite Time MDPs}{81}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1}Stochastic Fixed Point Iterations }{82}{section.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces \relax }}{86}{figure.caption.3}\protected@file@percent }
\newlabel{fig: Map}{{4.1}{86}{\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Contraction vs. no contraction}}{87}{figure.caption.4}\protected@file@percent }
\newlabel{fig: Map}{{4.2}{87}{Contraction vs. no contraction}{figure.caption.4}{}}
\newlabel{Fixed Theorem}{{1}{89}{Stochastic fixed point iteration for contractions on $\mathbb {R}^d$}{tcb@cnt@mydefinition.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Proof of the general stochastic fixed point iteration}{90}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Proof of boundedness}{90}{subsection.4.2.1}\protected@file@percent }
\newlabel{Wn goes to zero}{{2.1}{95}{}{tcb@cnt@mydefinition.4}{}}
\newlabel{first}{{4.1}{99}{Proof of boundedness}{equation.4.1}{}}
\newlabel{second}{{4.2}{99}{Proof of boundedness}{equation.4.2}{}}
\newlabel{third}{{4.3}{99}{Proof of boundedness}{equation.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Proof of convergence}{100}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Sample based dynamic Programming}{103}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Sample based policy evaluation algorithms}{105}{subsection.4.3.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {11}{\ignorespaces Totally asynchronous policy evaluation for $V^\pi $}}{107}{algocf.11}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {12}{\ignorespaces Totally asynchronous policy evaluation for $Q^\pi $}}{107}{algocf.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Q-learning and the SARSA trick}{108}{subsection.4.3.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {13}{\ignorespaces Q-learning}}{109}{algocf.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces \relax }}{111}{figure.caption.5}\protected@file@percent }
\newlabel{fig: Map}{{4.3}{111}{\relax }{figure.caption.5}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {14}{\ignorespaces SARSA}}{112}{algocf.14}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {15}{\ignorespaces n-Step SARSA}}{116}{algocf.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Others: Deep Q and alternate interpretation}{116}{subsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Gradient Descent Methods}{119}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1}Gradient descent for $L$-smooth functions}{120}{subsection.5.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2}Gradient descent for $L$-smooth convex functions}{122}{subsection.5.0.2}\protected@file@percent }
\newlabel{ConvexResultFromBefore}{{5.1}{125}{Gradient descent for $L$-smooth convex functions}{equation.5.1}{}}
\newlabel{Mu Strongly Convex}{{5.2}{125}{Gradient descent for $L$-smooth convex functions}{equation.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3}Gradient descent for $L$-smooth functions with PL inequality}{126}{subsection.5.0.3}\protected@file@percent }
\newlabel{Pl-applied to L-Smoothness}{{5.3}{128}{Gradient descent for $L$-smooth functions with PL inequality}{equation.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Non Convex function that satisfies the PL-inequality}}{129}{figure.caption.6}\protected@file@percent }
\newlabel{fig: Map}{{5.1}{129}{Non Convex function that satisfies the PL-inequality}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4}Gradient descent with diminishing step-sizes}{129}{subsection.5.0.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.5}Stochastic gradient descent}{132}{subsection.5.0.5}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {16}{\ignorespaces Plain vanilla stochastic gradient descent method (SGD)}}{132}{algocf.16}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {17}{\ignorespaces Plain vanilla policy gradient algorithm (exact gradients)}}{136}{algocf.17}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Policy gradient theorems}{137}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Finite-time MDPs}{137}{subsection.5.1.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {18}{\ignorespaces REINFORCE: (Batch-)Stochastic policy–gradient algorithm}}{142}{algocf.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Infinite-Time MDPs with discounted rewards}{143}{subsection.5.1.2}\protected@file@percent }
\newlabel{Assumptions on policy}{{1.2}{149}{}{tcb@cnt@mydefinition.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Convergence of REINFORCE}{150}{subsection.5.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Variance Reduction methods}{150}{subsection.5.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Important Exercises (29)}{153}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Deep Reinforcement Learning}{161}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1}Neural Networks}{161}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Distributional Reinforcement Learning}{161}{section.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {A}5 Minuten Über Fraktale der Value function}{163}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1}Paar Ideen}{165}{section.A.1}\protected@file@percent }
\ttl@finishall
\abx@aux@read@bbl@mdfivesum{76D65A242EC496C9B4361AF646FF12CB}
\gdef \@abspage@last{166}
