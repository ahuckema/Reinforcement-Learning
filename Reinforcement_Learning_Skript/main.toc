\babel@toc {english}{}\relax 
\contentsline {chapter}{Contents\ttl@gmk {\@mkboth {\MakeUppercase []{Contents}}{\MakeUppercase []{Contents}}}}{3}{chapter*.1}%
\contentsline {chapter}{\numberline {1}Stochastic Bandits}{5}{chapter.1}%
\contentsline {section}{\numberline {1}Two stage stochastic experiments}{5}{section.1.1}%
\contentsline {section}{\numberline {2}Introduction of stochastic Bandits}{6}{section.1.2}%
\contentsline {section}{\numberline {3}Algorithms: the exploration-exploitation trade-off}{14}{section.1.3}%
\contentsline {subsection}{\numberline {3.1}Basic committ-then-exploit algorithm}{14}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {3.2}Purely Greedy}{18}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {3.3}$\epsilon $-greedy bandit algorithms}{19}{subsection.1.3.3}%
\contentsline {subsection}{\numberline {3.4}UCB algorithm}{21}{subsection.1.3.4}%
\contentsline {subsection}{\numberline {3.5}Boltzmann exploration}{26}{subsection.1.3.5}%
\contentsline {subsection}{\numberline {3.6}Policy Gradient for Stochastic Bandits}{29}{subsection.1.3.6}%
\contentsline {chapter}{\numberline {2}Markov Decision Model}{35}{chapter.2}%
\contentsline {section}{\numberline {1}Markov Decision Problem}{35}{section.2.1}%
\contentsline {subsection}{\numberline {1.1}Markov-Chains}{35}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {1.2}Markov-Reward-Chains}{35}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {1.3}Markov-Decision Process}{37}{subsection.2.1.3}%
\contentsline {subsection}{\numberline {1.4}Stochastic Control theory}{45}{subsection.2.1.4}%
\contentsline {section}{\numberline {2}Basic tabular Value Iteration}{57}{section.2.2}%
\contentsline {section}{\numberline {3}Basic Policy Iteration (Actor Critic) algorithm}{61}{section.2.3}%
\contentsline {subsection}{\numberline {3.1}Policy Evaluation}{61}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {3.2}Policy Improvement}{65}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {3.3}Policy Iteration algorithms (tabular actor critic)}{69}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {3.4}Quick comparison of Value iteration and Policy Iteration}{70}{subsection.2.3.4}%
\contentsline {chapter}{\numberline {3}Simulation based dynamic Programming methods}{75}{chapter.3}%
\contentsline {section}{\numberline {1}A first example}{76}{section.3.1}%
\contentsline {section}{\numberline {2}Monte Carlo Policy evaluation and control}{76}{section.3.2}%
\contentsline {subsection}{\numberline {2.1}First Visit Monte Carlo Policy Evaluation}{77}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {2.2}Generalised policy iteration with first visit Monte Carlo estima- tion}{80}{subsection.3.2.2}%
\contentsline {chapter}{\numberline {4}Finite Time MDPs}{81}{chapter.4}%
\contentsline {section}{\numberline {1}Stochastic Fixed Point Iterations }{82}{section.4.1}%
\contentsline {section}{\numberline {2}Proof of the general stochastic fixed point iteration}{90}{section.4.2}%
\contentsline {subsection}{\numberline {2.1}Proof of boundedness}{90}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {2.2}Proof of convergence}{100}{subsection.4.2.2}%
\contentsline {section}{\numberline {3}Sample based dynamic Programming}{103}{section.4.3}%
\contentsline {subsection}{\numberline {3.1}Sample based policy evaluation algorithms}{105}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {3.2}Q-learning and the SARSA trick}{108}{subsection.4.3.2}%
\contentsline {subsection}{\numberline {3.3}Others: Deep Q and alternate interpretation}{116}{subsection.4.3.3}%
\contentsline {chapter}{\numberline {5}Gradient Descent Methods}{119}{chapter.5}%
\contentsline {subsection}{\numberline {0.1}Gradient descent for $L$-smooth functions}{120}{subsection.5.0.1}%
\contentsline {subsection}{\numberline {0.2}Gradient descent for $L$-smooth convex functions}{122}{subsection.5.0.2}%
\contentsline {subsection}{\numberline {0.3}Gradient descent for $L$-smooth functions with PL inequality}{126}{subsection.5.0.3}%
\contentsline {subsection}{\numberline {0.4}Gradient descent with diminishing step-sizes}{129}{subsection.5.0.4}%
\contentsline {subsection}{\numberline {0.5}Stochastic gradient descent}{132}{subsection.5.0.5}%
\contentsline {section}{\numberline {1}Policy gradient theorems}{137}{section.5.1}%
\contentsline {subsection}{\numberline {1.1}Finite-time MDPs}{137}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {1.2}Infinite-Time MDPs with discounted rewards}{143}{subsection.5.1.2}%
\contentsline {subsection}{\numberline {1.3}Convergence of REINFORCE}{150}{subsection.5.1.3}%
\contentsline {subsection}{\numberline {1.4}Variance Reduction methods}{150}{subsection.5.1.4}%
\contentsline {chapter}{\numberline {6}Important Exercises (29)}{153}{chapter.6}%
\contentsline {chapter}{\numberline {7}Deep Reinforcement Learning}{161}{chapter.7}%
\contentsline {section}{\numberline {1}Neural Networks}{161}{section.7.1}%
\contentsline {section}{\numberline {2}Distributional Reinforcement Learning}{161}{section.7.2}%
\contentsline {section}{\numberline {3}Deep Q-Networks}{162}{section.7.3}%
\contentsline {section}{\numberline {4}Proximal Policy Gradient}{166}{section.7.4}%
\contentsline {section}{\numberline {5}Advantage Estimation}{167}{section.7.5}%
\contentsline {section}{\numberline {6}Variants to PPO}{167}{section.7.6}%
\contentsline {section}{\numberline {7}Stable Baselines3}{167}{section.7.7}%
\contentsline {section}{\numberline {8}Hyperparameter Tuning}{167}{section.7.8}%
\contentsline {chapter}{\numberline {A}5 Minuten Ãœber Fraktale der Value function}{169}{appendix.A}%
\contentsline {section}{\numberline {1}Paar Ideen}{171}{section.A.1}%
\contentsfinish 
