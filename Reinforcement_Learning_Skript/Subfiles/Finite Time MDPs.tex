\documentclass[../main.tex]{subfiles}  % verweist auf die Masterâ€‘Datei
\begin{document}


\chapter{Finite Time MDPs}

Now the optimization goal is
$$
\pi^* \in \arg \max_{\pi} \mathbb{E}^\pi [\sum_{t=0}^T R_t]
$$
for fixed $T \in \mathbb{N}$. Here the idea of dynamic programming becomes much clearer, as its idea was to reduce a big problem into smaller subproblems. It will turn out the equations are backwards recursions, beginning from the End.\\
$\Pi^T_t$ denotes the set of polices $(\pi_i)_{i=t,...,T-1}$. 
Note that the following definition might seem a little bit cruel, but it is neseccary in order to avoid conditioning on an event with probability zero.
\defn{Value functions}{
For any $\pi \in \Pi^T_t$ we define $V_{T,T}\equiv 0$ and
$$
V_{t,T}^\pi (s) =\mathbb{E}^{\tilde{\pi}}_s [ \sum_{i=0}^{T-t-1} R_i]  =: \mathbb{E}^{\pi} [\sum_{i=t}^T R_i \mid S_t=s]
$$
the time-state value function from time $t$ to $T$, and $\tilde{\pi} := \pi_{\cdot + t}$ is the shifted policy by $t$. $V_{0,T}$ is the state value function.\\
We define $Q_{T,T} \equiv 0$ and
$$
Q_{t,T}^\pi(s,a)= \mathbb{E}^{\tilde{\pi}}_{s,a} [\sum_{i=0}^{T-1-t} R_i] =:  \mathbb{E}^{\pi} [\sum_{i=t}^T R_i \mid S_t=s , A_t=a].
$$
}

\prop{
Again it holds that
$$
V_{t,T}(s)= \sum_{a \in \mathcal{A}_s} \pi_t (a;s) Q_{t,T}(s,a)
$$
and the Q-V transfer
$$
Q_{t,T}^\pi(s,a) = r(s,a) + \gamma \sum_{s^\prime \in \mathcal{S}} p(s^\prime;s,a) V_{t+1,T}^\pi(s^\prime).
$$
The Bellman expectation operators are backwards recursions:
$$
V_{t,T}^\pi(s) = \sum_{a \in \mathcal{A}_s} \pi_t (a;s) (r(s,a)+  \sum_{s^\prime \in \mathcal{S}} p(s^\prime;s,a) V_{t+1;T}^\pi (s^\prime))
$$
and
$$
Q_{t,T}^\pi(s,a) =r(s,a)+  \sum_{s^\prime \in \mathcal{S}} \sum_{a \in \mathcal{A}_{s^\prime}} p(s^\prime;s,a) \pi_{t+1} (a^\prime;s^\prime) Q_{t+1,T}^\pi(s^\prime,a^\prime)
$$
}

The optimal value function now depends on the remaining time
\defn{Optimality}{
For all $t \leq T$\\
The optimal state value function 
$$
\forall s \in \mathcal{S}:\quad V_{t,T}^* (s) := \sup_{\pi \in \Pi^T_t} V_{t,T}^\pi(s)
$$
The optimal state action value function
$$
\forall (s,a) \in (\mathcal{S}\times \mathcal{A}):\quad Q_{t,T}^* (s,a) := \sup_{\pi \in \Pi^T_t} Q_{t,T}^\pi(s,a)
$$
A policy $\pi^*$ is optimal if its value function equals the optimal value function for every state and time:
$$
\forall s\in \mathcal{S}\; t\leq T:\quad V^{\pi^*}_{t,T} (s) = V^*_{t,T}(s)
$$
}

\thm{Dynamic Programming}{
Let $v_t: \mathcal{S} \to \mathbb{R}$ and $q_t : \mathcal{S} \times \mathcal{A} \to \mathbb{R}$ be seqeuneces $(v_t)_{t \leq T}$ and $(q_t)_{t \leq T}$ that satisfy the Bellman optimality equations as backwards recursions. Then it follows that 
$$
\forall t \leq T: \quad v_t =V^*_{t,T} \quad \text{and}\quad q_t =Q^*_{t,T}.
$$
Further, a optimal non stationary policy is given by the greedy polciy
$$
\pi_t (a;s) := \begin{cases}
    1&, a \in \arg \max_a q_t (s,a) \\
    0&, else.
\end{cases}
$$
}










\end{document}
