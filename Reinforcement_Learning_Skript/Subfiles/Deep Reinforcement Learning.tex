\documentclass[../main.tex]{subfiles}  % verweist auf die Masterâ€‘Datei
\begin{document}




\chapter{Deep Reinforcement Learning}

\section{Neural Networks}
... 
\section{Distributional Reinforcement Learning}
Remember that we defined $\mathbb{P}^\pi_{s,a} := \mathbb{P}^\pi \otimes \delta_{S_0}(s) \otimes \delta_{A_0}(a)$ as the probability measure of the Markov reward process $(S,A,R)$ started in $(s,a)$. We define the random variable of the return under policy $\pi$ as 
\[
Z^\pi := \sum_{t =0}^\infty \gamma^t R_t, \quad \gamma \in (0,1).
\]
Unlike the methods before, where we were interested in the expected reward $Q^\pi (s,a) = \mathbb{E}_{s,a}^\pi [Z^\pi]$, we are now interested in the distribution of these cumulative rewards. For that define
\[
\eta_{s,a}^\pi (B) := \mathbb{P}_{s,a}^\pi (Z^\pi \in  B), \quad B \in \mathcal{B}(\mathbb{R}).
\]
In analogy to weak solutions for PDEs, i.e. in sense of distribution, the return distribution $\eta_{s,a}^\pi$ satisfies the Bellman equation in sense of distribution:
\[
\forall \phi \in C_b(\mathbb{R}): \quad \int_{\mathbb{R}} \phi (z) d \eta_{s,a}^\pi (z) = \mathbb{E}_{s,a}^\pi [\int_\mathbb{R} \phi (R+\gamma z) d \eta_{S^\prime,A^\prime} (z) ].
\]
We define $f_{r,\gamma} (z) := r+\gamma z$ and the push forward 
\[
((\eta_{s,a}^\pi)_{f_{r,\gamma}}) (B) := \eta_{s,a}^\pi ( f^{-1}_{r,\gamma} ( B)), \quad B \in \mathcal{B}(\mathbb{R}).
\]
then, the above can be written as 
\begin{align*}
&\forall \phi \in C_b(\mathbb{R}): \quad \int_{\mathbb{R}} \phi (z) d \eta_{s,a}^\pi (z) = \mathbb{E}_{s,a}^\pi [\int_\mathbb{R} \phi (R+\gamma z) d \eta_{S^\prime,A^\prime} (z) ]= \mathbb{E}_{s,a}^\pi [ \int_\mathbb{R} \phi (z) d (\eta_{S^\prime,A^\prime})_{f_{R,\gamma}} (z)  ]\\
& \iff \forall \phi \in C_b(\mathbb{R}):\quad \langle \phi, \eta_{s,a}^\pi \rangle = \mathbb{E}_{s,a}^\pi [\langle \phi, (\eta_{S^\prime,A^\prime}^\pi)_{f_{R,\gamma}} \rangle].
\end{align*}

For any $\phi \in C_b(\mathbb{R})$ and any measure $\nu$ we have 
\begin{align*}
    (\nu * \delta_r) (\phi) &=\int_\mathbb{R} \phi (z) d (\nu * \delta_r) (z)= \int_\mathbb{R} \int_\mathbb{R} \phi (x+y) d \nu(x) d\delta_r(y) \\
    &= \int_\mathbb{R} \phi (x+r) d \nu (x) = (\nu (\cdot-r)) (\phi).
\end{align*}
Next, define $D_\gamma (x) := \gamma x$, then if $Z \sim \eta^\pi (x,a)$ then $\gamma Z\sim (\eta^\pi (s,a))_{D_\gamma}$. In total 
\[
Z\gamma +r \sim (\eta^\pi (s,a))_{D_\gamma} (\cdot-r) = ((\eta^\pi (s,a))_{D_\gamma} * \delta_r)
\]
holds in sense of distribution. Now define the distributional Bellman operator in sense of distribution as 
\[
\mathcal{T}^\pi Z^\pi (s,a) = R(s,a) + \gamma Z^\pi (S^\prime,A^\prime),
\]
i.e.  the distribution
\[
    \mathcal{T}^\pi: (\mathcal{P}(\mathbb{R}))^{\mathcal{S} \times \mathcal{A}} \to (\mathcal{P}(\mathbb{R}))^{\mathcal{S} \times \mathcal{A}}, (\nu_{s,a})_{(s,a) \in \mathcal{S} \times \mathcal{A}} \mapsto ((\mathcal{T}^\pi \nu)_{s,a})_{(s,a) \in \mathcal{S} \times \mathcal{A}} 
    \]
    is point wise defined for all $\phi \in C_b (\mathbb{R})$ as
\[
(\mathcal{T}^\pi \nu)_{s,a} (\phi) := \mathbb{E}_{s,a}^\pi [ ( (\nu (S^\prime,A^\prime))_{D_\gamma} * \delta_R ) (\phi) ] = \sum_{s^\prime,a^\prime,r} \pi (a^\prime;s^\prime) p(s^\prime,r;s,a) ((\nu_{s^\prime,a^\prime})_{D_\gamma} * \delta_r )(\phi)
\]
 

\section{Deep Q-Networks}
Deep Q-Networks (DQN) is a Q-learning variant that utilizes deep neural networks to approximate the Q-value. This algorithm addresses how to deal with large, continuous state action spaces $(\mathcal{S},\mathcal{A})$. The idea is to reduce dimensionality by plugging in the state space $\mathcal{S}$ (e.g. pixels in an Atari game) into a convolutional neural layer, which is the fed into a feed forward neural network that outputs the Q-value for each action $a \in \mathcal{A}$. Problems with simple neural networks are 
\begin{itemize}
    \item They can forget
    \item Are unstable: small changes in Q-values can lead to big changes in the policy, i.e. the action distribution 
    \item Correlation: In a trajectory $\tau_i$ of a rollout, the consecutive states $s_t,s_{t+1}$ are highly correlated, which violates the i.i.d. assumption for estimating the expectation.
    \item The loss is non-stationary, since the target values depend on the parameters of the network itself, meaning 
    \[
    \mathcal{L} (\theta) := \mathbb{E}[(\mathbb{E}[R+ \gamma \max_{a^\prime} Q^\theta (S^\prime,a^\prime)] - Q^\theta (S,A))^2],
    \]
    where both $Q$ terms depend on $\theta$.
\end{itemize}
In DQN these issues where solved by the following 
\begin{itemize}
    \item Experience Replay: A behavoiral policy $\pi^b$ samples Rollouts, that are stored in a buffer $\mathcal{D} = \{(s_i,a_i,r_i,s_i^\prime)\}_{i=1}^N$. In order to break the correlation between consecutive states, mini batches are sampled uniformly from this buffer to update the Q-network.
    \item Target Network: A second network with parameters $\theta^-$ is introduced, which is bootstrapped, i.e. every $C$ steps the parameters of the Q-network are copied to the target network $\theta^- \leftarrow \theta$. This delay stabilizes the training.
\end{itemize}
Tabular Q-learning can be interpreted in the following two ways: 
\begin{itemize}
    \item ML view: Fit a target $Q_n$ to the data $Q^{\pi^*}$. Define $Y_n := R + \gamma \max_{a^\prime \in \mathcal{A}} Q_n (S^\prime,a^\prime)$ and 
    \[
    \tilde{L}_n^Q := \mathbb{E} [ (\mathbb{E}_{S,A} [Y_n] - Q_n (S,A))^2 ] = \mathbb{E}[(Y_n - Q_n (S,A))^2] + \mathbb{E}[ \mathbb{V}_{S,A} (Y_n)]=: L_n^Q + \mathbb{E}[ \mathbb{V}_{S,A} (Y_n)].
    \]
    \item RL view: We want to approximate the Bellman optimality operator 
    \[
    T^* Q(s,a) := \mathbb{E}_{s,a} [R + \gamma \max_{a^\prime \in \mathcal{A}} Q(S^\prime,a^\prime)].
    \]
    Because it is a contraction in $\| \cdot \|_\infty$, the fixed point iteration converges.
\end{itemize}
Because the expectation is the minimizer of the $L^2$ error (variance), it holds that $ \mathbb{E}[X] = \arg \min_{\theta} \mathbb{E}[(X - \theta)^2]$ which leads to 
\[
T^* Q(s,a) = \mathbb{E}_{s,a} [R+\gamma \max_{a^\prime \in \mathcal{A}} Q(S^\prime ,a^\prime)] = \arg \min_{\theta} \mathbb{E}_{s,a} [(R + \gamma \max_{a^\prime \in \mathcal{A}} Q(S^\prime ,a^\prime) - \theta)^2].
\]
Thus doing one step SGD (with step size $\frac{\alpha}{2}$ ???) on $L_n^Q$ or doing the iteration for Q-learning yields the same scheme:
\[
Q_{n+1} (s,a) = Q_n (s,a) + \alpha (y-Q_n (s,a)).
\]
\rmkb{
    When doing Q-learning with neural networks, we no longer do the gradient on $(s,a) \in \mathcal{S}\times \mathcal{A}$, but instead on a smaller parameter space $\Theta \subset  \mathcal{S}\times \mathcal{A}$. Additionally, parameterizing the target with a different set of parameters $\theta^-$ the gradient of the loss becomes 
    \[
    \nabla_\theta L_n^Q = \mathbb{E} [ \nabla_\theta (Y_n^{\theta^-} - Q_n^\theta (S,A))^2 ] = -\mathbb{E} [ 2 (Y_n^{\theta^-} - Q_n^\theta (S,A)) \nabla_\theta Q_n^\theta (S,A) ].???-
    \]
    So the gradient step for SGD becomes 
    \[
    Q_{n+1}^\theta (s,a) = Q_n^\theta (s,a) + \alpha (y_n^{\theta^-} - Q_n^\theta (s,a)) \nabla_\theta Q_n^\theta (s,a).
    \]
}

\rmkb{
    In a convolutional neural network a convolutional layer is given by 
    \[
    (f_k * h) (i) = \sum_{j=-n}^n f_k(i-j) h(i),
    \]
    where $f_i$ is a filter $k=1,...,K$ and $h$ is some input vector. These $K$ filters operator simultaninously to and produce $(f_k * h) (i), k=1,...,K$ as output. $K$ is the number of Filters. In this setting the step size is called stride and was in the above $1$. In general it is 
    \[
    (f_k * h) (i) = \sum_{j=-n}^n f_k(i+s-j) h(i), \quad s \in \mathbb{N}.
    \]
}

\rmkb{
    For Atari games the Architecture is as follows:
    \begin{itemize}
        \item Preprocessing: The environment $84 \times 84$ pixels is converted into grayscale. Let $x_t \in \mathbb{R}^{H\times W}$ be a greyscale image at time $t$, where $H=W=84$. To create a sense of motion four consecutive images are stacked: $s_t := (x_t,...,x_{t-3}) \in \mathbb{R}^{H\times W \times 4}$. Together with the scores, this is fed into the neural network.
        \item Convolutional Layers: There are three convolutional layers used. Each convolutional layers is followed by a ReLU activation function. The first layer has 32 filters of size $8 \times 8$ with stride $4$. The second layer has 64 filters of size $4 \times 4$ with stride $2$. The third layer has 64 filters of size $3 \times 3$ with stride $1$.
        \item Feed Forward Layers: The output of the last convolutional layer is flattened and fed into a fully connected layer with 512 units, followed by a ReLU activation function.
        \item Output: The final layer in the network is a fully connected layer that outputs a vector of Q-values, one for each possible action in the Atari game.
    \end{itemize}
}


\rmkb{
    \begin{itemize}
        \item Experience Replay: This solves two problems: From the buffer $\mathcal{D}$ mini batches are chosen following an $\epsilon$-greedy policy, which solves the problem of the correlation between rollouts and the policy. Further, it also solves the problem, that neural networks are prone to forget old data. \\
        If the buffer is full, the oldest transitions are removed first.
        \item Target Networks:
        \begin{itemize}
            \item Different Parametrization: The target distribution is parametrized with $\theta^-$ Every $N$ steps it is updated with a soft update rule 
        \[
        \theta^- \leftarrow \tau \theta^- + (1-\tau) \theta.
        \]
        This stabilizes training: 
        \[
        \tilde{L}_n (\theta) := \mathbb{E}[ \left( \mathbb{E}_{S,A} [Y_n^{\theta^-}] - Q_n^\theta (s,a)  \right)^2 ] = L_n (\theta) + \mathbb{E}[ \mathbb{V}_{S,A} [Y_n^{\theta^-}] ],
        \]
        because $\tilde{L}_n (\theta)$ no longer depends on the expected variance.
        \item Update every $N$ steps: With the second point of view of $Q$ learning, updating every $N$ steps is actually $N$ step SGD of 
        \[
        T^* Q^{\theta^-} (s,a) = \mathbb{E}_{s,a} [R+\gamma Q^{\theta^-} (S^\prime,a^\prime) ] = \arg \min_{\theta} \mathbb{E}[(Y_n^{\theta^-} - \theta)^2].
        \]
        \end{itemize} 
    \end{itemize}
}
\begin{algorithm}[H]
\SetKwInput{Data}{Data}
\SetKwInput{Result}{Result}
\SetAlgoLined
\DontPrintSemicolon

\Data{Replay buffer capacity $|D|$, discount factor $\gamma$, exploration rate $\varepsilon$, target update frequency $N$}
\Result{Trained Q-network approximating $Q^*$}

Initialise replay buffer $D$\;
Initialise Q-network with random weights $\theta$\;
Copy weights to target network: $\theta^- \leftarrow \theta$\;

\For{each episode}{
    Pre-process the initial observation $s_1$ to obtain state representation $\phi_1$\;
    \For{each time-step $t$}{
        Select action $a_t$ using $\varepsilon$-greedy policy based on $Q(\phi_t, a; \theta)$\;
        Execute action $a_t$, observe reward $r_t$ and next observation $s_{t+1}$\;
        Pre-process $s_{t+1}$ to obtain $\phi_{t+1}$\;
        Store transition $(\phi_t, a_t, r_t, \phi_{t+1})$ in replay buffer $D$\;
        Sample mini-batch $B$ of transitions from $D$\;
        \For{each transition $i \in B$}{
            Compute target: \\
            $Y_i = \begin{cases} r_i, & \text{if terminal transition,} \\ r_i + \gamma \max_{a'} Q(\phi'_i, a'; \theta^-), & \text{otherwise.} \end{cases}$
        }
        Perform gradient descent on loss:
        \[ L_B(\theta) = \frac{1}{|B|} \sum_{i \in B} (Y_i - Q(\phi_i, a_i; \theta))^2 \]
        Update Q-network weights $\theta$\;
        \If{every $N$ steps}{
            Update target network: $\theta^- \leftarrow \theta$\;
        }
        However, target networks increase memory, as it another neural network needs to be stored and updating the weights increases computational costs.
    }
}
\caption{The Deep Q-Network (DQN) algorithm.}
\end{algorithm}
\rmkb{
    \begin{itemize}
        \item Double Q-learning and Double DQN: Q-learning over estimates the action values due to the max. operator, In Double Q-learning we maintain two independent networks $Q^{A}$ and $Q^B$ to estimate $Q^{\pi^*}$:
        \begin{align*}
           & Q^A (s,a) \leftarrow (1-\alpha) Q^A (s,a) + \alpha \left( r + \gamma Q^B (s^\prime , \arg \max_{a^\prime \in \mathcal{A}} Q^{A} (s^\prime, a^\prime )) \right)\\
            &=(1-\alpha) Q^A (s,a) + \alpha \left( r + \gamma Q^A (s^\prime , \arg \max_{a^\prime \in \mathcal{A}} Q^{A} (s^\prime, a^\prime )) \right) \\
            &+\gamma \left( Q^B (s^\prime,\arg \max_{a^\prime \in \mathcal{A}} (s^\prime,a^\prime)) - Q^{A} (s^\prime,\arg \max_{a^\prime \in \mathcal{A}} Q^{A} (s^\prime,a^\prime)) \right).
        \end{align*}
        In Double Q-learning the Networks are updated in every step and independendly. In Double DQN we give one network the role of the target network and the other the Q-network. The Q-network is updated every step. The target network is updated only every N step with the soft update rule, which depends on the Q-network. So indepence and simultaninous updates do not happen.
        \item Clipped Double Q/TD3: Another approach is to clip the bias term (third summand in the above) by taking its negative part: $b^- := \min \{b,0\}$. The deep variant of this method is called Twin delayed double Deep Q Networks.
    \end{itemize}
}




\section{Proximal Policy Gradient}

\section{Advantage Estimation}

\section{Variants to PPO}

\section{Stable Baselines3}


\end{document}