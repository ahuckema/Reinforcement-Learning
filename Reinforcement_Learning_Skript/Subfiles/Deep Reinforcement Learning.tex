\documentclass[../main.tex]{subfiles}  % verweist auf die Masterâ€‘Datei
\begin{document}




\chapter{Deep Reinforcement Learning}

\section{Neural Networks}
... 
\section{Distributional Reinforcement Learning}
Remember that we defined $\mathbb{P}^\pi_{s,a} := \mathbb{P}^\pi \otimes \delta_{S_0}(s) \otimes \delta_{A_0}(a)$ as the probability measure of the Markov reward process $(S,A,R)$ started in $(s,a)$. We define the random variable of the return under policy $\pi$ as 
\[
Z^\pi := \sum_{t =0}^\infty \gamma^t R_t, \quad \gamma \in (0,1).
\]
Unlike the methods before, where we were interested in the expected reward $Q^\pi (s,a) = \mathbb{E}_{s,a}^\pi [Z^\pi]$, we are now interested in the distribution of these cumulative rewards. For that define
\[
\eta_{s,a}^\pi (B) := \mathbb{P}_{s,a}^\pi (Z^\pi \in  B), \quad B \in \mathcal{B}(\mathbb{R}).
\]
In analogy to weak solutions for PDEs, i.e. in sense of distribution, the return distribution $\eta_{s,a}^\pi$ satisfies the Bellman equation in sense of distribution:
\[
\forall \phi \in C_b(\mathbb{R}): \quad \int_{\mathbb{R}} \phi (z) d \eta_{s,a}^\pi (z) = \mathbb{E}_{s,a}^\pi [\int_\mathbb{R} \phi (R+\gamma z) d \eta_{S^\prime,A^\prime} (z) ].
\]
We define $f_{r,\gamma} (z) := r+\gamma z$ and the push forward 
\[
((\eta_{s,a}^\pi)_{f_{r,\gamma}}) (B) := \eta_{s,a}^\pi ( f^{-1}_{r,\gamma} ( B)), \quad B \in \mathcal{B}(\mathbb{R}).
\]
then, the above can be written as 
\begin{align*}
&\forall \phi \in C_b(\mathbb{R}): \quad \int_{\mathbb{R}} \phi (z) d \eta_{s,a}^\pi (z) = \mathbb{E}_{s,a}^\pi [\int_\mathbb{R} \phi (R+\gamma z) d \eta_{S^\prime,A^\prime} (z) ]= \mathbb{E}_{s,a}^\pi [ \int_\mathbb{R} \phi (z) d (\eta_{S^\prime,A^\prime})_{f_{R,\gamma}} (z)  ]\\
& \iff \forall \phi \in C_b(\mathbb{R}):\quad \langle \phi, \eta_{s,a}^\pi \rangle = \mathbb{E}_{s,a}^\pi [\langle \phi, (\eta_{S^\prime,A^\prime}^\pi)_{f_{R,\gamma}} \rangle].
\end{align*}

For any $\phi \in C_b(\mathbb{R})$ and any measure $\nu$ we have 
\begin{align*}
    (\nu * \delta_r) (\phi) &=\int_\mathbb{R} \phi (z) d (\nu * \delta_r) (z)= \int_\mathbb{R} \int_\mathbb{R} \phi (x+y) d \nu(x) d\delta_r(y) \\
    &= \int_\mathbb{R} \phi (x+r) d \nu (x) = (\nu (\cdot-r)) (\phi).
\end{align*}
Next, define $D_\gamma (x) := \gamma x$, then if $Z \sim \eta^\pi (x,a)$ then $\gamma Z\sim (\eta^\pi (s,a))_{D_\gamma}$. In total 
\[
Z\gamma +r \sim (\eta^\pi (s,a))_{D_\gamma} (\cdot-r) = ((\eta^\pi (s,a))_{D_\gamma} * \delta_r)
\]
holds in sense of distribution. Now define the distributional Bellman operator in sense of distribution as 
\[
\mathcal{T}^\pi Z^\pi (s,a) = R(s,a) + \gamma Z^\pi (S^\prime,A^\prime),
\]
i.e.  the distribution
\[
    \mathcal{T}^\pi: (\mathcal{P}(\mathbb{R}))^{\mathcal{S} \times \mathcal{A}} \to (\mathcal{P}(\mathbb{R}))^{\mathcal{S} \times \mathcal{A}}, (\nu_{s,a})_{(s,a) \in \mathcal{S} \times \mathcal{A}} \mapsto ((\mathcal{T}^\pi \nu)_{s,a})_{(s,a) \in \mathcal{S} \times \mathcal{A}} 
    \]
    is point wise defined for all $\phi \in C_b (\mathbb{R})$ as
\[
(\mathcal{T}^\pi \nu)_{s,a} (\phi) := \mathbb{E}_{s,a}^\pi [ ( (\nu (S^\prime,A^\prime))_{D_\gamma} * \delta_R ) (\phi) ] = \sum_{s^\prime,a^\prime,r} \pi (a^\prime;s^\prime) p(s^\prime,r;s,a) ((\nu_{s^\prime,a^\prime})_{D_\gamma} * \delta_r )(\phi)
\]
 

\section{Deep Q-Networks}
Deep Q-Networks (DQN) is a Q-learning variant that utilizes deep neural networks to approximate the Q-value. This algorithm addresses how to deal with large, continuous state action spaces $(\mathcal{S},\mathcal{A})$. The idea is to reduce dimensionality by plugging in the state space $\mathcal{S}$ (e.g. pixels in an Atari game) into a convolutional neural layer, which is the fed into a feed forward neural network that outputs the Q-value for each action $a \in \mathcal{A}$. Problems with simple neural networks are 
\begin{itemize}
    \item They can forget
    \item Are unstable: small changes in Q-values can lead to big changes in the policy, i.e. the action distribution 
    \item Correlation: In a trajectory $\tau_i$ of a rollout, the consecutive states $s_t,s_{t+1}$ are highly correlated, which violates the i.i.d. assumption for estimating the expectation.
    \item The loss is non-stationary, since the target values depend on the parameters of the network itself, meaning 
    \[
    \mathcal{L} (\theta) := \mathbb{E}[(\mathbb{E}[R+ \gamma \max_{a^\prime} Q^\theta (S^\prime,a^\prime)] - Q^\theta (S,A))^2],
    \]
    where both $Q$ terms depend on $\theta$.
\end{itemize}
In DQN these issues where solved by the following 
\begin{itemize}
    \item Experience Replay: A behavoiral policy $\pi^b$ samples Rollouts, that are stored in a buffer $\mathcal{D} = \{(s_i,a_i,r_i,s_i^\prime)\}_{i=1}^N$. In order to break the correlation between consecutive states, mini batches are sampled uniformly from this buffer to update the Q-network.
    \item Target Network: A second network with parameters $\theta^-$ is introduced, which is bootstrapped, i.e. every $C$ steps the parameters of the Q-network are copied to the target network $\theta^- \leftarrow \theta$. This delay stabilizes the training.
\end{itemize}



\section{Proximal Policy Gradient}

\section{Advantage Estimation}

\subsection{Variants to PPO}

\section{Stable Baselines3}


\end{document}